# Measuring-stereotypical-bias-in-Russian-language-models

Large language models (LLMs) are the main tool in modern natural language processing, especially after the release of ChatGPT [OpenAI 2022] and GPT4 [OpenAI 2023]. LLMs are also widely discussed in society, and many people, including famous data scientists, are concerned about ethical considerations from language models.

[Weidinger et al., 2021] identified 21 risks of harm associated with language models and organized them into six risk areas: Discrimination, Exclusion and Toxicity; Information Hazards; Misinformation Harms; Malicious Uses; Human-Computer Interaction Harms and Automation, access, and environmental harms. Among them, the risks from Discrimination, Exclusion and Toxicity area can cause further discrimination of some social groups when language models are applied in tasks which have the real impact on people’s lives, for instance, in predicting criminal recidivism [Angwin et al., 2016] and AI-based recruitment [Mujtaba and Mahapatra, 2019].

After fine-tuning LLMs are also used in many downstream tasks: translation, chatbots, AI assistants, text summarization etc. Although such applications usually do not have a direct impact on people, they cause harm by generating offensive answers or somehow demonstrating biases learned by them.

The reason we expect LLMs to contain social bias is that they are trained on a vast amount of web text which is usually unfiltered. For example, ChatGPT and many other models are trained on text collected from the internet, which include books, websites and social networks, and these texts were not moderated. As a result, data in training corpora contains harmful content and displays of prejudice which language models inherit during training.

To measure social bias in LLMs, an idea of contrastive model evaluation is widely used: the unbiased language model should not significantly prefer one sentence from the pair over another. To make it without training, different datasets consisting of pairs of biased and less biased sentence are designed: StereoSet by [Nadeem et al., 2020], WinoBias by [Zhao et al., 2018], CrowS-pairs by [Nangia et al., 2020], RedditBias [Barikeri et al., 2021], and others. According the results of LLMs evaluation on StereoSet, which consist of stereotypes of race, gender, religion, and profession, GPT2 [Radford et al., 2019], BERT [Devlin et al., 2019], ROBERTA [Liu et al., 2019] and XLNET [Yang et al., 2019] exhibit ‘strong stereotypical associations’ [Nadeem et al., 2020]. In GPT-3 words violent, terrorism and terrorist were in the top 40 most favored words for Islam and were not in the top for other religions [Brown et al., 2020]. Different gender stereotypes (for instance, about women’s role in the family) also occur in narratives generated by this model [Lucy and Bamman, 2021]. Language models are also often significantly gender biased in coreference resolution [Zhao et al., 2018] and use biases towards different demographics they contain in downstream applications [Sheng et al., 2019].

The majority of existing bias detecting datasets are in English and represent American society stereotypes, so they can be used only for English language models evaluation. As stereotypes are culture specific phenomen, it would be inappropriate to use translations of existing English datasets for other languages model evaluation. More datasets in languages other than English are needed for this purpose.
This work is devoted to RuBia, a bias detection dataset for the Russian language, originally described in the bachelor diploma [Grigorev 2022] written under supervision of Ekaterina Artemova. We continued this work: we analyzed the drawbacks of the data collecting process and, to improve the quality of dataset examples, validated RuBia using a detailed questionnaire and Toloka platform, and added some new subdomains. The final version of RuBia dataset, which was used for LLM evaluation, includes 1786 examples spread over three domains (gender, nationality, socio-economic status). Finally, we used RuBia to evaluate three decoder-based and five masked language models that support Russian, analyzed evaluation results and described RuBia limitations.

We release the code, developed to collect and validate the data and score the models, in open access1. We also publish validated version of RuBia, validation instructions and questions2.
